# Smart Chunking v0.2: Deep Technical Documentation

## Overview

The **Smart Chunking System v0.2** is an intelligent file processing pipeline that handles legacy COBOL files of any size (tested up to 999,999 lines of code) while maintaining **100% code consistency and integrity**. The system uses semantic boundary detection, SQLite-backed consistency registries, and parallel processing with rate limiting to ensure no code is ever truncated or lost during AI-powered migration.

### Key Capabilities
- **Automatic size detection**: Files routed to direct or chunked processing based on configurable thresholds
- **Semantic boundary detection**: Chunks split at COBOL division/section/paragraph boundaries
- **Cross-chunk consistency**: SQLite-backed SignatureRegistry and TypeMappingTable ensure consistent naming
- **Parallel processing**: Up to 6 chunks processed concurrently with rate limiting
- **Forward reference resolution**: References to not-yet-converted code handled gracefully
- **Progressive context compression**: Older chunk summaries compressed to stay within token budgets
- **No truncation policy**: System fails loudly if chunk too large, NEVER silently drops code

### AI Models Used
| Component | Model | Purpose |
|-----------|-------|---------|
| CobolAnalyzerAgent | `gpt-5.1-codex-mini` | Structural analysis of COBOL files |
| BusinessLogicExtractorAgent | `gpt-5.1-codex-mini` | Extract user stories, features, business rules |
| JavaConverterAgent | `gpt-5.1-codex-mini` | Convert COBOL to Quarkus Java |
| CSharpConverterAgent | `gpt-5.1-codex-mini` | Convert COBOL to C# |
| Portal Q&A | `gpt-5.1-chat` | Interactive migration Q&A |

---

## Architecture Components

### Core Classes
| Class | File | Purpose |
|-------|------|---------|
| `SmartMigrationOrchestrator` | Processes/SmartMigrationOrchestrator.cs | Entry point - routes files to direct/chunked processing |
| `ChunkedMigrationProcess` | Processes/ChunkedMigrationProcess.cs | Full chunked conversion pipeline |
| `ChunkedReverseEngineeringProcess` | Processes/ChunkedReverseEngineeringProcess.cs | Chunked analysis/business logic extraction |
| `ChunkingOrchestrator` | Chunking/ChunkingOrchestrator.cs | Coordinates chunking operations |
| `SemanticUnitChunker` | Chunking/Core/SemanticUnitChunker.cs | Semantic boundary detection |
| `CobolAdapter` | Chunking/Adapters/CobolAdapter.cs | COBOL-specific parsing |
| `SignatureRegistry` | Chunking/Core/SignatureRegistry.cs | SQLite method signature consistency |
| `TypeMappingTable` | Chunking/Core/TypeMappingTable.cs | SQLite variable type consistency |
| `ChunkContextManager` | Chunking/Context/ChunkContextManager.cs | Builds context for each chunk |

---

## Pipeline Flow: SmartMigrationOrchestrator

The `SmartMigrationOrchestrator` is the entry point that routes files to the appropriate processing path:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SMART MIGRATION ORCHESTRATOR                                  â”‚
â”‚                    (SmartMigrationOrchestrator.cs)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚   Source Files â†’ CategorizeFiles() â†’ Size Check                                 â”‚
â”‚                                                                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚         â”‚   RequiresChunking() = false   â”‚  RequiresChunking() = true  â”‚        â”‚
â”‚         â–¼                                â–¼                              â”‚        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚        â”‚
â”‚   â”‚   SMALL FILE    â”‚            â”‚   LARGE FILE    â”‚                   â”‚        â”‚
â”‚   â”‚ < 150K chars    â”‚            â”‚ > 150K chars    â”‚                   â”‚        â”‚
â”‚   â”‚ < 3,000 lines   â”‚            â”‚ > 3,000 lines   â”‚                   â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚        â”‚
â”‚            â”‚                              â”‚                             â”‚        â”‚
â”‚            â–¼                              â–¼                             â”‚        â”‚
â”‚   ProcessSmallFilesOnlyAsync()   ProcessLargeFilesAsync()              â”‚        â”‚
â”‚   (MigrationProcess)             (ChunkedMigrationProcess)             â”‚        â”‚
â”‚                                                                         â”‚        â”‚
â”‚   âš¡ Direct conversion            ğŸ“¦ Smart chunking with                â”‚        â”‚
â”‚   (Single API call per agent)     consistency guarantees               â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Threshold Configuration (ChunkingSettings.cs)

```csharp
public class ChunkingSettings
{
    // Primary auto-detection thresholds
    public int AutoChunkCharThreshold { get; set; } = 150_000;   // 150K characters
    public int AutoChunkLineThreshold { get; set; } = 3_000;     // 3,000 lines

    // Chunk sizing
    public int MaxLinesPerChunk { get; set; } = 1500;            // Lines per chunk
    public int OverlapLines { get; set; } = 300;                 // Context overlap
    public int MaxTokensPerChunk { get; set; } = 28_000;         // Token budget

    // Parallel processing
    public int MaxParallelChunks { get; set; } = 6;              // Conversion workers
    public int MaxParallelAnalysis { get; set; } = 6;            // Analysis workers
    public bool EnableParallelProcessing { get; set; } = true;

    // Rate limiting
    public int TokenBudgetPerMinute { get; set; } = 300_000;     // Azure TPM limit
    public double RateLimitSafetyFactor { get; set; } = 0.7;     // 70% utilization

    // Progressive compression
    public double CompressionRatio { get; set; } = 0.3;          // 30% of original
    public int FullDetailChunkWindow { get; set; } = 3;          // Full detail for last 3 chunks

    // Helper method used by orchestrator
    public bool RequiresChunking(int charCount, int lineCount)
    {
        return charCount > AutoChunkCharThreshold || lineCount > AutoChunkLineThreshold;
    }
}
```

---

## Phase 1: Reverse Engineering (Analysis)

### Small Files (< 150K chars, < 3K lines)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SMALL FILE REVERSE ENGINEERING                                                   â”‚
â”‚                                                                                  â”‚
â”‚   file.cbl (50K chars)                                                          â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ CobolAnalyzerAgent â”‚ â”€â”€â–º â”‚ BusinessLogic      â”‚ â”€â”€â–º â”‚ Output:          â”‚   â”‚
â”‚   â”‚ (Single API call)  â”‚     â”‚ ExtractorAgent     â”‚     â”‚ Analysis +       â”‚   â”‚
â”‚   â”‚ gpt-5.1-codex-mini â”‚     â”‚ (Single API call)  â”‚     â”‚ Business Rules   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                  â”‚
â”‚   Total API Calls: 2                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Large Files (> 150K chars OR > 3K lines)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LARGE FILE REVERSE ENGINEERING (ChunkedReverseEngineeringProcess)               â”‚
â”‚                                                                                  â”‚
â”‚   bigfile.cbl (400K chars, 10K lines)                                           â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ STEP 1: CHUNKING ORCHESTRATOR (ChunkingOrchestrator.cs)                â”‚    â”‚
â”‚   â”‚                                                                        â”‚    â”‚
â”‚   â”‚   Settings: MaxLinesPerChunk=1500, OverlapLines=300                   â”‚    â”‚
â”‚   â”‚                                                                        â”‚    â”‚
â”‚   â”‚   10,000 lines Ã· (1,500 - 300 overlap) = ~8 chunks                    â”‚    â”‚
â”‚   â”‚   Each chunk: 1,500 lines with 300-line overlap for context           â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ STEP 2: SEMANTIC BOUNDARY DETECTION (SemanticUnitChunker.cs)           â”‚    â”‚
â”‚   â”‚                                                                        â”‚    â”‚
â”‚   â”‚   Uses CobolAdapter to identify natural split points:                  â”‚    â”‚
â”‚   â”‚   - IDENTIFICATION DIVISION                                            â”‚    â”‚
â”‚   â”‚   - DATA DIVISION / WORKING-STORAGE SECTION                           â”‚    â”‚
â”‚   â”‚   - PROCEDURE DIVISION sections/paragraphs                            â”‚    â”‚
â”‚   â”‚                                                                        â”‚    â”‚
â”‚   â”‚   Splits at semantic boundaries when possible                          â”‚    â”‚
â”‚   â”‚   Falls back to line-based if a single unit is too large              â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ Chunk 0  â”‚ Chunk 1  â”‚ Chunk 2  â”‚ Chunk 3  â”‚ Chunk 4  â”‚ Chunk 5  â”‚ Chunk 6â”‚ â”‚
â”‚   â”‚ Lines    â”‚ Lines    â”‚ Lines    â”‚ Lines    â”‚ Lines    â”‚ Lines    â”‚ Lines  â”‚ â”‚
â”‚   â”‚ 1-1500   â”‚ 1201-2700â”‚ 2401-3900â”‚ 3601-5100â”‚ 4801-6300â”‚ 6001-7500â”‚ 7201+  â”‚ â”‚
â”‚   â”‚ (overlap)â”‚ (overlap)â”‚ (overlap)â”‚ (overlap)â”‚ (overlap)â”‚ (overlap)â”‚        â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚          â”‚          â”‚          â”‚          â”‚          â”‚         â”‚       â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                         â”‚                                        â”‚
â”‚                                         â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ STEP 3: PARALLEL PROCESSING (MaxParallelAnalysis=6)                     â”‚   â”‚
â”‚   â”‚                                                                         â”‚   â”‚
â”‚   â”‚   SemaphoreSlim limits concurrent API calls                            â”‚   â”‚
â”‚   â”‚   Stagger delay: 500ms between chunk starts                            â”‚   â”‚
â”‚   â”‚                                                                         â”‚   â”‚
â”‚   â”‚   For each chunk (parallel up to 6):                                   â”‚   â”‚
â”‚   â”‚   1. CobolAnalyzerAgent.AnalyzeAsync() â†’ Structure analysis            â”‚   â”‚
â”‚   â”‚   2. BusinessLogicExtractorAgent.ExtractAsync() â†’ Business rules       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ STEP 4: MERGE RESULTS (MergeAnalyses + MergeBusinessLogics)             â”‚   â”‚
â”‚   â”‚                                                                         â”‚   â”‚
â”‚   â”‚   MergeAnalyses():                                                      â”‚   â”‚
â”‚   â”‚   - Combine all DataDivisions (deduplicated by name)                   â”‚   â”‚
â”‚   â”‚   - Combine all ProcedureDivisions (deduplicated)                      â”‚   â”‚
â”‚   â”‚   - Combine all Paragraphs (by unique name)                            â”‚   â”‚
â”‚   â”‚   - Combine all Variables (by unique name)                             â”‚   â”‚
â”‚   â”‚   - Combine all CopybooksReferenced                                    â”‚   â”‚
â”‚   â”‚                                                                         â”‚   â”‚
â”‚   â”‚   MergeBusinessLogics():                                                â”‚   â”‚
â”‚   â”‚   - Combine UserStories with unique IDs (US-1, US-2, ...)             â”‚   â”‚
â”‚   â”‚   - Combine Features with unique IDs (F-1, F-2, ...)                  â”‚   â”‚
â”‚   â”‚   - Combine BusinessRules with unique IDs (BR-1, BR-2, ...)           â”‚   â”‚
â”‚   â”‚   - Deduplicate rules by description to avoid duplicates              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   Single unified reverse-engineering-details.md                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 2: Code Conversion (Migration)

### Small Files (Direct Conversion)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SMALL FILE CONVERSION (MigrationProcess)                                         â”‚
â”‚                                                                                  â”‚
â”‚   file.cbl â†’ ChunkAwareJavaConverter OR ChunkAwareCSharpConverter â†’ file.java   â”‚
â”‚              (Single API call with gpt-5.1-codex-mini)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Large Files (ChunkedMigrationProcess)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LARGE FILE CONVERSION (ChunkedMigrationProcess)                                  â”‚
â”‚                                                                                  â”‚
â”‚   bigfile.cbl (400K chars)                                                      â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â”‚
â”‚   â•‘ PHASE 1: BUILD SYMBOL TABLE (CobolAdapter - Before any AI calls)       â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   Parse the ENTIRE file using regex patterns to extract:               â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   Regex Patterns Used:                                                 â•‘    â”‚
â”‚   â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘    â”‚
â”‚   â•‘   â”‚ Division Pattern:                                              â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ ^\s*(IDENTIFICATION|DATA|PROCEDURE|ENVIRONMENT)\s+DIVISION     â”‚  â•‘    â”‚
â”‚   â•‘   â”‚                                                                â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ Section Pattern:                                               â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ ^\s*(WORKING-STORAGE|FILE|LINKAGE|LOCAL-STORAGE)\s+SECTION     â”‚  â•‘    â”‚
â”‚   â•‘   â”‚                                                                â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ Paragraph Pattern:                                             â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ ^\s+([A-Z0-9-]+)\s*\.\s*$                                       â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ (Starts with whitespace, alphanumeric-dash name, ends with .)  â”‚  â•‘    â”‚
â”‚   â•‘   â”‚                                                                â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ COPY Pattern: COPY\s+([A-Z0-9-]+)                              â”‚  â•‘    â”‚
â”‚   â•‘   â”‚ CALL Pattern: CALL\s+['"]([A-Z0-9-]+)['"]                      â”‚  â•‘    â”‚
â”‚   â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   Extracts:                                                            â•‘    â”‚
â”‚   â•‘   - All variables with types (CUST-ID PIC 9(8) â†’ long)                â•‘    â”‚
â”‚   â•‘   - All paragraphs/sections with line ranges                          â•‘    â”‚
â”‚   â•‘   - All COPY statements (copybook references)                         â•‘    â”‚
â”‚   â•‘   - All CALL statements (external program calls)                      â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   Stored in SQLite-backed registries:                                 â•‘    â”‚
â”‚   â•‘   - SignatureRegistry: method signatures                              â•‘    â”‚
â”‚   â•‘   - TypeMappingTable: variableâ†’type mappings                          â•‘    â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â”‚
â”‚   â•‘ PHASE 2: CREATE CHUNKS (SemanticUnitChunker)                           â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   Token Estimation: chars / 4 (CharsPerToken = 4)                     â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   Priority Order for Chunking:                                        â•‘    â”‚
â”‚   â•‘   1. Semantic boundaries (divisions, sections, paragraphs)            â•‘    â”‚
â”‚   â•‘   2. Line-based fallback if single unit > MaxLinesPerChunk            â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   Each chunk contains:                                                â•‘    â”‚
â”‚   â•‘   - ChunkIndex (0-based)                                              â•‘    â”‚
â”‚   â•‘   - StartLine, EndLine (1-based)                                      â•‘    â”‚
â”‚   â•‘   - Content (actual COBOL code)                                       â•‘    â”‚
â”‚   â•‘   - SemanticUnitNames (paragraphs/sections in this chunk)            â•‘    â”‚
â”‚   â•‘   - EstimatedTokens                                                   â•‘    â”‚
â”‚   â•‘   - HasOverlap (true if overlaps with previous chunk)                â•‘    â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â”‚
â”‚   â•‘ PHASE 3: PARALLEL CONVERSION (Rate-limited via SemaphoreSlim)          â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   MaxParallelChunks: 3 workers (conversion is heavier than analysis)  â•‘    â”‚
â”‚   â•‘   TokenBudgetPerMinute: 300,000 (Azure OpenAI TPM)                    â•‘    â”‚
â”‚   â•‘   RateLimitSafetyFactor: 0.7 (use only 70% of budget)                â•‘    â”‚
â”‚   â•‘   StaggerDelay: 1000ms between chunk starts                           â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   For each chunk:                                                      â•‘    â”‚
â”‚   â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘    â”‚
â”‚   â•‘   â”‚ ChunkContextManager.BuildContextAsync() provides:             â”‚   â•‘    â”‚
â”‚   â•‘   â”‚                                                               â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ 1. FileSummary - what this file does overall                 â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ 2. PreviousSignatures - all methods converted so far         â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ 3. TypeMappings - all variableâ†’type mappings                 â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ 4. PendingForwardReferences - refs to not-yet-converted code â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ 5. CompressedHistory - summaries of older chunks             â”‚   â•‘    â”‚
â”‚   â•‘   â”‚                                                               â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ Progressive Compression:                                      â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ - Last 3 chunks: full detail (FullDetailChunkWindow=3)       â”‚   â•‘    â”‚
â”‚   â•‘   â”‚ - Older chunks: compressed to 30% (CompressionRatio=0.3)     â”‚   â•‘    â”‚
â”‚   â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   AI Prompt includes:                                                 â•‘    â”‚
â”‚   â•‘   - ## VARIABLE TYPES (from TypeMappingTable)                        â•‘    â”‚
â”‚   â•‘   - ## ALREADY CONVERTED METHODS (from SignatureRegistry)            â•‘    â”‚
â”‚   â•‘   - ## CODE TO CONVERT (this chunk's COBOL)                          â•‘    â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â”‚
â”‚   â•‘ PHASE 4: VALIDATION & RECONCILIATION                                   â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   SignatureRegistry.ValidateSignatureAsync():                         â•‘    â”‚
â”‚   â•‘   - Check if method signature already exists                          â•‘    â”‚
â”‚   â•‘   - Compare TargetMethodName, ReturnType, Parameters                  â•‘    â”‚
â”‚   â•‘   - Report discrepancies with severity (Error/Warning)                â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   ReconcileChunks() validates:                                        â•‘    â”‚
â”‚   â•‘   âœ“ All method signatures match across all chunks                    â•‘    â”‚
â”‚   â•‘   âœ“ All variable types are consistent                                â•‘    â”‚
â”‚   â•‘   âœ“ All cross-references are valid                                   â•‘    â”‚
â”‚   â•‘   âœ“ No forward references remain unresolved                          â•‘    â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â”‚
â”‚   â•‘ PHASE 5: ASSEMBLY (Multi-file output)                                  â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   For Java:                                                            â•‘    â”‚
â”‚   â•‘   - Detect multiple classes in converted code                         â•‘    â”‚
â”‚   â•‘   - Generate separate .java files per class                           â•‘    â”‚
â”‚   â•‘   - Add proper package and import statements                          â•‘    â”‚
â”‚   â•‘   - Add Quarkus annotations (@ApplicationScoped, @Transactional)     â•‘    â”‚
â”‚   â•‘                                                                        â•‘    â”‚
â”‚   â•‘   For C#:                                                              â•‘    â”‚
â”‚   â•‘   - Generate namespace structure                                       â•‘    â”‚
â”‚   â•‘   - Use partial classes if needed for large files                     â•‘    â”‚
â”‚   â•‘   - Add proper using statements                                       â•‘    â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚        â”‚                                                                         â”‚
â”‚        â–¼                                                                         â”‚
â”‚   Output: Multiple .java or .cs files + chunked-migration-report.md             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Deep Dive: Consistency Mechanisms

### 1. SignatureRegistry (SQLite-Backed)

The `SignatureRegistry` ensures that method signatures are **immutable once registered**. This prevents the AI from generating different signatures for the same COBOL paragraph across chunks.

**Database Schema:**
```sql
CREATE TABLE signatures (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    source_file TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,
    legacy_name TEXT NOT NULL,           -- Original COBOL paragraph name
    target_method_name TEXT NOT NULL,    -- Java/C# method name
    target_signature TEXT NOT NULL,      -- Full signature string
    return_type TEXT NOT NULL,           -- Return type
    parameters TEXT,                     -- JSON array of parameters
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(run_id, source_file, legacy_name)
);
```

**Registration Flow:**
```csharp
// From SignatureRegistry.cs
public async Task<MethodSignature> RegisterSignatureAsync(...)
{
    // 1. Check if signature already exists
    var existing = await GetSignatureAsync(runId, sourceFile, signature.LegacyName, ...);
    if (existing != null)
    {
        // Return EXISTING signature - AI must use this
        return existing;
    }

    // 2. Insert new signature with ON CONFLICT DO NOTHING
    // This ensures first-writer wins in parallel scenarios
    await using var command = connection.CreateCommand();
    command.CommandText = @"
        INSERT INTO signatures (...) VALUES (...)
        ON CONFLICT(run_id, source_file, legacy_name) DO NOTHING;
        SELECT ... FROM signatures WHERE ...;";

    // 3. Return the registered (or existing) signature
}
```

**Validation Flow:**
```csharp
// SignatureValidationResult checks for discrepancies
public async Task<SignatureValidationResult> ValidateSignatureAsync(...)
{
    var existing = await GetSignatureAsync(...);
    if (existing == null) return new SignatureValidationResult { IsValid = true };

    // Check for discrepancies
    if (existing.TargetMethodName != signature.TargetMethodName)
    {
        result.Discrepancies.Add(new SignatureDiscrepancy
        {
            Field = "TargetMethodName",
            ExpectedValue = existing.TargetMethodName,
            ActualValue = signature.TargetMethodName,
            Severity = DiscrepancySeverity.Error
        });
    }
    // Also checks: ReturnType, Parameters
}
```

**Example:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SIGNATURE REGISTRY IN ACTION                                                     â”‚
â”‚                                                                                  â”‚
â”‚   Chunk 1 converts VALIDATE-CUSTOMER:                                           â”‚
â”‚   â†’ AI generates: public boolean validateCustomer(long customerId)              â”‚
â”‚   â†’ RegisterSignatureAsync() stores in SQLite                                   â”‚
â”‚   â†’ LOCKED: legacy_name="VALIDATE-CUSTOMER", target="validateCustomer"          â”‚
â”‚                                                                                  â”‚
â”‚   Chunk 5 references VALIDATE-CUSTOMER:                                         â”‚
â”‚   â†’ ChunkContextManager includes: "validateCustomer(long customerId)"           â”‚
â”‚   â†’ AI sees the registered signature and MUST use it                            â”‚
â”‚   â†’ If AI generates different signature â†’ ValidateSignatureAsync fails          â”‚
â”‚                                                                                  â”‚
â”‚   Result: 100% consistent method names and signatures across all chunks         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. TypeMappingTable (SQLite-Backed)

The `TypeMappingTable` ensures consistent variable-to-type mappings across all chunks.

**Database Schema:**
```sql
CREATE TABLE type_mappings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    source_file TEXT NOT NULL,
    legacy_variable TEXT NOT NULL,    -- Original COBOL variable name
    legacy_type TEXT NOT NULL,        -- PIC clause (e.g., "PIC 9(8)")
    target_type TEXT NOT NULL,        -- Java/C# type (e.g., "long")
    target_field_name TEXT NOT NULL,  -- camelCase field name
    is_nullable INTEGER DEFAULT 0,
    default_value TEXT,
    UNIQUE(run_id, source_file, legacy_variable)
);
```

**Type Inference Logic (from TypeMappingTable.cs):**
```csharp
public string InferTargetType(string legacyType, TargetLanguage targetLanguage)
{
    // COBOL PIC clause patterns:
    // PIC 9(n)        â†’ int/long based on digits
    // PIC S9(n)V9(m)  â†’ decimal/BigDecimal (has decimal places)
    // PIC X(n)        â†’ string/String (alphanumeric)
    // PIC A(n)        â†’ string/String (alphabetic)
    // COMP/BINARY     â†’ int

    var normalizedType = legacyType.ToUpperInvariant().Trim();

    if (pic.Contains('V') || pic.Contains('.'))
    {
        // Decimal type for values with decimal places
        return isCSharp ? "decimal" : "BigDecimal";
    }

    if (pic.Contains('9') && !pic.Contains('X'))
    {
        var digits = CountDigits(pic);
        if (digits <= 4) return "short";
        if (digits <= 9) return "int";
        if (digits <= 18) return "long";
        return isCSharp ? "decimal" : "BigDecimal";
    }

    // Default to string for X and A types
    return isCSharp ? "string" : "String";
}
```

**Example Mappings:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TYPE MAPPING TABLE EXAMPLES                                                      â”‚
â”‚                                                                                  â”‚
â”‚   COBOL Variable        Legacy Type        â†’    Java/C# Type                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚   CUST-ID               PIC 9(8)           â†’    long customerId                 â”‚
â”‚   CUST-NAME             PIC X(50)          â†’    String customerName             â”‚
â”‚   CUST-BAL              PIC S9(9)V99       â†’    BigDecimal customerBalance      â”‚
â”‚   ORDER-COUNT           PIC 9(4)           â†’    short orderCount                â”‚
â”‚   ACTIVE-FLAG           PIC X(1)           â†’    String activeFlag               â”‚
â”‚                                                                                  â”‚
â”‚   This table is:                                                                â”‚
â”‚   1. Pre-computed BEFORE any AI calls                                           â”‚
â”‚   2. LOCKED once registered                                                     â”‚
â”‚   3. Passed to EVERY chunk as context                                           â”‚
â”‚   4. AI cannot invent different types                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3. ChunkContextManager

The `ChunkContextManager` builds rich context for each chunk's AI prompt:

```csharp
// From ChunkContextManager.cs
public async Task<ChunkContext> BuildContextAsync(
    int runId,
    string sourceFile,
    int currentChunkIndex,
    IReadOnlyList<ChunkResult> previousResults,
    string? fileSummary = null,
    CancellationToken cancellationToken = default)
{
    var context = new ChunkContext
    {
        SourceFile = sourceFile,
        ChunkIndex = currentChunkIndex,
        FileSummary = fileSummary ?? "",

        // Get all registered signatures from SQLite
        PreviousSignatures = await _signatureRegistry
            .GetAllSignaturesAsync(runId, sourceFile, cancellationToken),

        // Get all type mappings from SQLite
        TypeMappings = await _typeMappingTable
            .GetAllMappingsAsync(runId, sourceFile, cancellationToken),

        // Identify forward references (calls to not-yet-converted code)
        PendingForwardReferences = IdentifyForwardReferences(
            previousResults, currentChunkIndex),

        // Compress older chunk summaries to save tokens
        CompressedHistory = CompressHistory(previousResults, currentChunkIndex)
    };

    return context;
}
```

**Progressive Compression:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROGRESSIVE COMPRESSION (FullDetailChunkWindow=3, CompressionRatio=0.3)          â”‚
â”‚                                                                                  â”‚
â”‚   Processing Chunk 7:                                                            â”‚
â”‚                                                                                  â”‚
â”‚   Chunk 0: "IDENTIFICATION DIVISION..." â†’ Compressed to ~30% (summary only)     â”‚
â”‚   Chunk 1: "DATA DIVISION vars..."      â†’ Compressed to ~30% (summary only)     â”‚
â”‚   Chunk 2: "PROCEDURE SECTION-A..."     â†’ Compressed to ~30% (summary only)     â”‚
â”‚   Chunk 3: "SECTION-B processing..."    â†’ Compressed to ~30% (summary only)     â”‚
â”‚   Chunk 4: [Full detail - within window]                                        â”‚
â”‚   Chunk 5: [Full detail - within window]                                        â”‚
â”‚   Chunk 6: [Full detail - within window]                                        â”‚
â”‚   Chunk 7: [CURRENT - being converted]                                          â”‚
â”‚                                                                                  â”‚
â”‚   This keeps token usage manageable while preserving recent context             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4. Forward Reference Resolution

When code in Chunk 3 calls a paragraph that will be converted in Chunk 7:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FORWARD REFERENCE HANDLING                                                       â”‚
â”‚                                                                                  â”‚
â”‚   Chunk 3 code: PERFORM CALCULATE-TOTALS                                        â”‚
â”‚   (CALCULATE-TOTALS is in Chunk 7, not yet converted)                           â”‚
â”‚                                                                                  â”‚
â”‚   1. ChunkContextManager detects this as a forward reference                    â”‚
â”‚   2. AI is told: "CALCULATE-TOTALS will be converted later"                    â”‚
â”‚   3. AI generates: // TODO: Call calculateTotals() when available              â”‚
â”‚                                                                                  â”‚
â”‚   After Chunk 7 completes:                                                      â”‚
â”‚   1. CALCULATE-TOTALS â†’ calculateTotals(List<Order> orders)                    â”‚
â”‚   2. Registered in SignatureRegistry                                            â”‚
â”‚   3. Reconciliation phase resolves the forward reference                        â”‚
â”‚   4. Final code: calculateTotals(orders);                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5. No Truncation Policy

The system **NEVER silently drops code**:

```csharp
// If a chunk exceeds limits, the system FAILS with a clear error
if (chunkCharCount > _settings.AutoChunkCharThreshold)
{
    throw new InvalidOperationException(
        $"âŒ CHUNK TOO LARGE: Chunk {chunkIndex} has {chunkCharCount:N0} chars " +
        $"(max: {_settings.AutoChunkCharThreshold:N0}).\n" +
        $"Suggested fix: Reduce MaxLinesPerChunk from {_settings.MaxLinesPerChunk} " +
        $"to {_settings.MaxLinesPerChunk * 80 / 100} in appsettings.json");
}
```

---

## Semantic Boundary Detection (CobolAdapter)

The `CobolAdapter` uses regex patterns to identify natural split points in COBOL code:

```csharp
// From CobolAdapter.cs
public class CobolAdapter : ILanguageAdapter
{
    // Division detection
    private static readonly Regex DivisionPattern = new(
        @"^\s*(IDENTIFICATION|DATA|PROCEDURE|ENVIRONMENT)\s+DIVISION",
        RegexOptions.IgnoreCase | RegexOptions.Multiline);

    // Section detection  
    private static readonly Regex SectionPattern = new(
        @"^\s*(WORKING-STORAGE|FILE|LINKAGE|LOCAL-STORAGE|COMMUNICATION|REPORT|SCREEN)\s+SECTION",
        RegexOptions.IgnoreCase | RegexOptions.Multiline);

    // Paragraph detection (name followed by period at start of line)
    private static readonly Regex ParagraphPattern = new(
        @"^\s+([A-Z0-9][A-Z0-9-]*)\s*\.\s*$",
        RegexOptions.Multiline);

    // External references
    private static readonly Regex CopyPattern = new(
        @"COPY\s+([A-Z0-9-]+)",
        RegexOptions.IgnoreCase);

    private static readonly Regex CallPattern = new(
        @"CALL\s+['""]([A-Z0-9-]+)['""]",
        RegexOptions.IgnoreCase);

    public SemanticUnit[] ExtractSemanticUnits(string content)
    {
        var units = new List<SemanticUnit>();
        var lines = content.Split('\n');

        // 1. Find all divisions
        foreach (Match match in DivisionPattern.Matches(content))
        {
            units.Add(new SemanticUnit
            {
                UnitType = SemanticUnitType.Division,
                Name = match.Groups[1].Value.ToUpper() + " DIVISION",
                StartLine = GetLineNumber(content, match.Index),
                // EndLine computed based on next unit
            });
        }

        // 2. Find all sections
        foreach (Match match in SectionPattern.Matches(content))
        {
            units.Add(new SemanticUnit
            {
                UnitType = SemanticUnitType.Section,
                Name = match.Groups[1].Value.ToUpper() + " SECTION",
                StartLine = GetLineNumber(content, match.Index),
            });
        }

        // 3. Find all paragraphs (only in PROCEDURE DIVISION)
        // ... paragraph extraction logic
    }
}
```

**Semantic Unit Priority:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHUNK SPLIT PRIORITY                                                             â”‚
â”‚                                                                                  â”‚
â”‚   1. DIVISION boundaries (strongest)                                            â”‚
â”‚      - IDENTIFICATION DIVISION                                                  â”‚
â”‚      - DATA DIVISION                                                            â”‚
â”‚      - PROCEDURE DIVISION                                                       â”‚
â”‚                                                                                  â”‚
â”‚   2. SECTION boundaries                                                         â”‚
â”‚      - WORKING-STORAGE SECTION                                                  â”‚
â”‚      - FILE SECTION                                                             â”‚
â”‚      - Named sections in PROCEDURE DIVISION                                     â”‚
â”‚                                                                                  â”‚
â”‚   3. PARAGRAPH boundaries                                                       â”‚
â”‚      - Named paragraphs (VALIDATE-CUSTOMER, PROCESS-ORDER, etc.)               â”‚
â”‚                                                                                  â”‚
â”‚   4. Line-based fallback                                                        â”‚
â”‚      - If a single unit exceeds MaxLinesPerChunk                               â”‚
â”‚      - Split at line boundaries with overlap                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Database Schema (SQLite)

The chunking system uses SQLite for persistence across all runs:

```sql
-- Chunk processing metadata
CREATE TABLE chunk_metadata (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    source_file TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,
    start_line INTEGER NOT NULL,
    end_line INTEGER NOT NULL,
    status TEXT DEFAULT 'Pending',        -- Pending, Processing, Completed, Failed
    semantic_units TEXT,                  -- JSON array of unit names
    tokens_used INTEGER DEFAULT 0,
    processing_time_ms INTEGER DEFAULT 0,
    error_message TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    completed_at DATETIME,
    UNIQUE(run_id, source_file, chunk_index)
);

-- Method signatures (immutable after registration)
CREATE TABLE signatures (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    source_file TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,
    legacy_name TEXT NOT NULL,
    target_method_name TEXT NOT NULL,
    target_signature TEXT NOT NULL,
    return_type TEXT NOT NULL,
    parameters TEXT,                      -- JSON array
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(run_id, source_file, legacy_name)
);

-- Variable type mappings (immutable after registration)
CREATE TABLE type_mappings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    source_file TEXT NOT NULL,
    legacy_variable TEXT NOT NULL,
    legacy_type TEXT NOT NULL,
    target_type TEXT NOT NULL,
    target_field_name TEXT NOT NULL,
    is_nullable INTEGER DEFAULT 0,
    default_value TEXT,
    UNIQUE(run_id, source_file, legacy_variable)
);

-- Forward references (resolved after all chunks complete)
CREATE TABLE forward_references (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    source_file TEXT NOT NULL,
    from_chunk INTEGER NOT NULL,
    to_chunk INTEGER,
    reference_name TEXT NOT NULL,
    reference_type TEXT NOT NULL,         -- Method, Variable
    resolved INTEGER DEFAULT 0,
    resolved_signature TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

---

## Configuration Reference (appsettings.json)

```json
{
  "ChunkingSettings": {
    "AutoChunkCharThreshold": 150000,
    "AutoChunkLineThreshold": 3000,
    "MaxLinesPerChunk": 1500,
    "OverlapLines": 300,
    "MaxTokensPerChunk": 28000,
    "MaxParallelChunks": 6,
    "MaxParallelAnalysis": 6,
    "EnableParallelProcessing": true,
    "TokenBudgetPerMinute": 300000,
    "RateLimitSafetyFactor": 0.7,
    "CompressionRatio": 0.3,
    "FullDetailChunkWindow": 3
  },
  "RateLimitSettings": {
    "TokensPerMinute": 300000,
    "MinDelayBetweenRequestsMs": 2000
  }
}
```

---

## Error Handling & Troubleshooting

### Error Messages

| Error | Cause | Solution |
|-------|-------|----------|
| `âŒ FILE TOO LARGE: file.cbl has 409,448 chars (max: 150,000)` | File exceeds single-call limit | System auto-routes to chunked processing |
| `âŒ CHUNK TOO LARGE: Chunk X has Y chars (max: 150,000)` | Individual chunk still too big | Reduce `MaxLinesPerChunk` in appsettings.json |
| `HTTP 429 (RateLimitReached)` | Too many API calls per minute | Increase `MinDelayBetweenRequestsMs` or reduce `MaxParallelChunks` |
| `âŒ Signature mismatch for VALIDATE-CUSTOMER` | AI generated different signature | Check SignatureRegistry, may need manual fix |

### Troubleshooting Steps

1. **File Too Large â†’ Uses Chunking Automatically**
   ```bash
   # SmartMigrationOrchestrator auto-detects and routes
   dotnet run --source source
   ```

2. **Chunk Too Large Error**
   ```bash
   # Edit appsettings.json
   "MaxLinesPerChunk": 1000  # Reduce from 1500 to 1000
   "OverlapLines": 200        # Reduce proportionally
   ```

3. **Rate Limit (429) Error**
   ```bash
   # Edit appsettings.json
   "MinDelayBetweenRequestsMs": 5000  # Increase delay to 5 seconds
   "MaxParallelChunks": 2              # Reduce parallel workers
   ```

4. **Check Chunking Health**
   ```bash
   ./doctor.sh chunking-health
   ```

5. **Monitor Progress in Real-Time**
   ```bash
   # CLI
   ./helper-scripts/track-progress.sh --watch

   # Portal (Enhanced v0.3)
   open http://localhost:5028
   # â€¢ Live Activity dashboard now embedded in left panel
   # â€¢ View Target Language (Java â˜• / C# âš™ï¸) in run selector
   # â€¢ Real-time tracking of Active Workers and Phases
   # Click "ğŸ”„ Migration Monitor" for deep-dive chunk analysis
   ```

---

## Summary Tables

### File Size â†’ Processing Path
| File Size | Processing | API Calls | Output |
|-----------|-----------|-----------|--------|
| **< 150K chars AND < 3K lines** | Direct (MigrationProcess) | 2-3 per file | Single file |
| **> 150K chars OR > 3K lines** | Chunked (ChunkedMigrationProcess) | 2 per chunk | Multiple files |

### Consistency Mechanisms
| Mechanism | Storage | Purpose |
|-----------|---------|---------|
| **SignatureRegistry** | SQLite `signatures` table | Lock method signatures after first conversion |
| **TypeMappingTable** | SQLite `type_mappings` table | Lock variableâ†’type mappings |
| **ChunkContextManager** | In-memory + SQLite | Build context for each chunk's AI prompt |
| **Forward References** | SQLite `forward_references` table | Track calls to not-yet-converted code |
| **Reconciliation** | Post-processing | Validate all chunks are consistent |
| **No Truncation Policy** | Runtime check | Fail loudly if chunk too large |

### Parallel Processing Configuration
| Setting | Default | Purpose |
|---------|---------|---------|
| `MaxParallelAnalysis` | 6 | Workers for reverse engineering |
| `MaxParallelChunks` | 3 | Workers for code conversion |
| `TokenBudgetPerMinute` | 300,000 | Azure OpenAI TPM limit |
| `RateLimitSafetyFactor` | 0.7 | Use 70% of budget for safety margin |
| `StaggerDelay` | 500-1000ms | Delay between parallel chunk starts |
